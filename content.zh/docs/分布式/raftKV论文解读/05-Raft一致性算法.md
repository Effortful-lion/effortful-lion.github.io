---
title: "05-Raft一致性算法"
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: true
# bookComments: false
# bookSearchExclude: false
# bookHref: ''
# bookIcon: ''
---

# Raft 一致性算法

### Raft算法的简略设计

主要是算法角色、方法、参数、算法过程的定义：下面有个图分别解释了对全局节点（以及特定节点）的属性、RPC调用的方法和参数以及逻辑过程

![](/assets/Raft一致性算法-1769421830382.png)
![](/assets/Raft一致性算法-1769422012709.png)

![](/assets/Raft一致性算法-1769421863938.png)
![](/assets/Raft一致性算法-1769421988140.png)

![](/assets/Raft一致性算法-1769421876478.png)
![](/assets/Raft一致性算法-1769421944960.png)

![](/assets/Raft一致性算法-1769421892424.png)
![](/assets/Raft一致性算法-1769422042502.png)

### Raft特性
![](/assets/Raft一致性算法-1769422088144.png)
简记：
- 领导人唯一
- 领导人只能日志追加
- 日志匹配（有相同日志内容的节点）
- 提交日志不丢失（通过主节点的选举规则，强制新主节点 “继承所有已提交日志”）
- 如果某一服务器已将给定索引位置的日志条目应用至其状态机中，则其他任何服务器在该索引位置不会应用不同的日志条目（已经应用的日志条目不会被覆盖）

## Raft基础

### Raft的状态/角色

- 领导者
- 候选人
- 跟随着

### 角色职责

- 领导者：1. 提供服务 2. 心跳检测 3. 日志复制
- 候选人：1. 发起投票
- 跟随着：1. 投票 2. 日志复制

### 状态/角色转换关系

服务器状态。跟随者只响应来自其他服务器的请求。如果跟随者接收不到消息，那么他就会
变成候选人并发起一次选举。获得集群中大多数选票的候选人将成为领导人。在一个任期内，领导
人一直都会是领导人，直到自己宕机了：

![](/assets/05-Raft一致性算法-1769426488547.png)
或者看：
![](/assets/05-Raft一致性算法-1769427227899.png)

任期时间：
- 任期连续 —— 用连续的序号表示
- 任期从选举开始，从任期/选举失败（无领导人）结束（意味着：一个任期里领导者为 0 或者 1）
- 任期号全局递增
- 节点的任期号（每次通信，发现比自己大的）自动更新（候选人/领导人会退回跟随者）
- 发现任期号小的请求，直接拒绝
![](/assets/05-Raft一致性算法-1769427451957.png)

### 节点间通信形式/方法

形式：RPC

方法（2+1）：
1. 请求投票：候选人发起，跟随者响应，选举期间
2. 附加条目：leader发起，跟随者响应（正常情况下，一个集群中只有 Leader + Follower）（leader失联，follower发起选举）
3. 传输快照：（并行发送请求 + 无响应请求重试）== 效率 + 可靠

### 领导选举过程

...

### 日志复制

1. leader 提供服务时：
- 记录日志
- 发起RPC附加日志
- 当被安全复制后，应用到自己的状态机将结果返回到客户端、同步所有跟随者的日志（重试保证所有跟随者最终都存储了该日志）**无限后台异步重试（不会丢弃）**

2. committed entries：已提交条目（一致性判定标志）表示已被多数节点确认的日志条目，不会丢失、修改、覆盖，可以安全应用到状态机执行
![](/assets/05-Raft一致性算法-1769429869983.png)
提交时间：leader创建的日志条目被多数派节点复制成功后，就会被提交（当前日志条目和之前所有的日志条目）

3. 日志匹配特性：
- 如果在**不同的日志**中的两个条目拥有**相同的索引和任期号**，那么他们**存储了相同的指令**。（leader在一个任期中的指定的一个索引位置最多创建一条日志，并且日志位置不会改变）
- 如果在**不同的日志**中的两个条目拥有**相同的索引和任期号**，那么他们**之前的所有日志条目也全部相同**（一致性检查：leader发送附加日志时携带：前一个日志的任期+索引号）

4. 正常情况下，领导人和跟随者的日志保持一致性，所以附加日志 RPC 的一致性检查从来不会失败。但是leader的崩溃，可能会使日志出现不一致的状态：（甚至在一系列的leader/follower崩溃下加剧）：
![](/assets/05-Raft一致性算法-1769431051813.png)
新leader和follower之间的日志条目相互都有出入
![](/assets/05-Raft一致性算法-1769431137257.png)
![](/assets/05-Raft一致性算法-1769431300022.png)
每个任期的最小索引处进行一致性校验，替代从第一个nextIndex开始每个索引处进行一致性校验：
![](/assets/05-Raft一致性算法-1769431385662.png)
所以，新leader只需要正常操作进行附加条目的RPC，通过RPC时的一致性校验，自动进行检查后就会慢慢相同。同时：
![](/assets/05-Raft一致性算法-1769431616717.png)

### 安全性

前面描述了 选举 和 日志复制，但是仍然存在问题：

1. 我们的选举规则： 超时控制+FIFO的选举规则+多数人选举+随机选举时间 
2. 但是，如果此时让一个日志很少的节点获得了leader，那么根据一致性检验就会覆盖其他节点的可提交日志条目。
3. 但是，可提交日志条目很有可能被部分节点应用到状态机（尤其是旧leader应用后且响应给客户端），此时就会存在数据不一致性（不同客户端看到了不同的数据）

#### 选举限制

领导人完全特性：这是提出的解决上述问题的机制。目的选出日志最新的节点作为leader（继承前面leader的日志）。

方式：raft的日志方向唯一（leader -> follower）所以不会将旧日志从follower传送到新leader（说白了，就是直接选出来最新的）
1. 发起投票 RPC 时，请求中包含候选者的最新日志信息（任期号+索引号）
2. 跟随者不是 简单的**FIFO** ，而是 **FIFO+新旧日志校验** ，如果 候选者 的**任期号小**或者**任期号相同 + 索引小**，那么 跟随者 拒绝投票

---

#### 提交之前任期的日志条目

概念：
1. 日志回滚：就是未提交的日志，撤销提交操作（或者说直接覆盖/删除）。（简单说，这个未提交日志没了）

背景：
1. 旧任期（比如任期 2）的日志 2，被 Leader 通过多数派确认直接标记为已提交（图 c 中 S1、S2、S5 有日志 2，满足多数派）；
2. 但此时 Leader（S1）又崩溃，新 Leader（S5）的日志里没有日志 2（S5 只有 1、3）；
3. 新 Leader（S5）在同步时，会通过一致性检查，把 S1、S2、S3、S4 中已提交的日志 2覆盖 / 删除（图 d 中日志 2 被替换成 3）；
4. 结果：已提交的日志 2 被回滚，破坏了 “已提交日志永不丢失” 的安全性。

解决办法：
1. 禁止直接提交旧任期日志：不再通过多数派确认直接提交旧任期（如任期 2）的日志 2；
2. 只直接提交当前任期日志：新 Leader（S1 恢复后）生成当前任期（比如任期 4）的新日志 4，通过多数派确认直接提交日志 4；
3. 间接提交旧日志：由于日志匹配特性（前缀必须一致），日志 4 提交时，其前面的所有日志（包括旧任期的日志 2）会被连带间接提交；
4. 结果：日志 2 被 “绑定” 到当前任期的已提交日志 4 上，后续任何 Leader 都必须包含日志 4（选举规则），因此也必须包含日志 2，日志 2 永远不会被回滚（图 e 中日志 2 被保留，日志 4 被提交）。

图示：问题的产生和解决（对raft论文的原图理解）
![](/assets/05-Raft一致性算法-1769436423976.png)

---

#### 安全性论证

**这里是论证：** 已提交的日志，永远不会丢失，所有未来的 Leader 都一定包含它。

**回忆：**领导人完全性（就是 **新leader一定继承旧leader的提交了的日志条目** / **集群中所有已经提交（committed）的日志条目**）

**证明方法：**反证法，通过假设不存在领导人完全性

**假设：**
- 任期 T 的 Leader（记为 Leader T）提交了一条日志 L
- 但存在某个未来任期 U（U > T）的 Leader（Leader U），它的日志里没有 L
- 并且 U 是第一个不包含 L 的 Leader（最小的 U > T）（任期U就是第一个不包含已提交日志的未来的leader）

**推出矛盾：**这样的leader不可能存在

**根据背景可以推出已知：**
1. 选举时 Leader U 没有 L（因为 Leader 从不删 / 覆盖日志）
   Leader 只会追加日志，不会删除或覆盖
   所以 Leader U 当选时，日志里就没有 L
2. 两个 “多数派” 必有交集（核心数学点）
   Leader T 提交 L → L 被复制到多数派节点（记为集合 A）
   Leader U 当选 → 从多数派节点获得选票（记为集合 B）
   两个多数派集合 A、B 必有交集（数学上：任意两个多数派一定相交）
   所以：至少有一个节点 S，既收到了 L，又给 Leader U 投了票
   这个 S 就是 “矛盾的关键节点”。
3. S 投票给 U 时，一定还保存着 L
   S 先收到 Leader T 的 L（因为 L 已提交）
   S 给 U 投票前，任期 ≤ U
   中间任何 Leader 都包含 L（因为 U 是第一个不包含 L 的 Leader）
   Leader 从不删日志，Follower 只在和 Leader 冲突时才删
   所以：S 投票给 U 时，日志里一定还有 L

投票条件：U 的日志必须 ≥ S 的日志（新旧判定）

矛盾点：
1. U == S
- 既然U得到票了 => U的长度>=S
- 但是 S有之前的+L ，U有之前的但是没有L（并且不能有日志空洞），所以U一定要有L。才至少使得长度和S相等。那么矛盾了（我们假设的是U没有L）

2. U > S
- U > S,（U是第一个没有L的）那么前一个leader V一定有L，U >= V >= S ,回到1了，U也必须有L才能大于V

---

#### 跟随者和候选者崩溃

二者处理方式相同：
1. 直接崩溃，领导者无限重试RPC
2. 执行了RPC，还没来得及响应崩溃了；领导者重试RPC（无所谓，RPC幂等，再次执行，不会出现重复日志条目，直接忽略）

---

#### 时间和可用性

**Raft 时间相关核心规则简洁总结**
1. **核心原则**：安全性与时间无关（不会因事件快慢出错误），可用性（需要选举成功）依赖时间（时间不满足则无法稳定提供服务）；
2. **选举稳定的核心时间不等式**：**广播时间（心跳机制） ≪ 选举超时时间 ≪ 平均故障间隔时间**（三个时间需相差一个及以上数量级）；
3. **各时间定义**
    - 广播时间：单节点并行发RPC给集群所有节点并接收响应的平均时间；
    - 选举超时时间：跟随者触发选举的超时阈值；
    - 平均故障间隔时间：单服务器两次故障的平均间隔；
4. **不等式的意义**
    - 广播时间远小于选举超时：Leader能及时发心跳阻止跟随者发起选举，避免无意义竞选；
    - 选举超时远小于平均故障间隔：系统因Leader崩溃导致的不可用时间（约等于选举超时）占比极低，保证整体稳定性；
5. **时间配置特点**
    - 广播时间、平均故障间隔由系统/硬件决定，选举超时可自主配置；
    - 实际配置参考：广播时间0.5~20ms，选举超时10~500ms，服务器平均故障间隔多为数月及以上，易满足要求。


### 集群成员变化

**背景**

实际场景中需要修改集群配置（比如换故障机器、增减节点），不能直接停集群改配置（会不可用+易出错），所以Raft把配置变更自动化、纳入一致性算法。

**核心问题**

不能直接从旧配置切到新配置！核心问题是：直接切换时，所有节点没法同时完成配置更新，会出现“新旧配置并存”的过渡期——此时集群并非只有一个统一的多数派标准，新旧配置的多数派会分别生效，进而诞生双Leader，导致数据混乱。你可能会疑惑“按道理一个集群只有一个多数派”，关键就在于“配置未统一”：这里的「配置」，对Raft集群而言，核心是「参与一致性算法的服务器集合」（比如节点的IP/标识、集群总节点数），以及基于这个集合衍生的规则（比如多数派票数标准、节点是否有投票权/参与日志复制的权限）。多数派的判定标准，是由节点当前使用的配置决定的（旧配置有旧的多数派规则，新配置有新的规则），过渡期内节点按不同标准判定“多数派是否达标”，自然会出现两个多数派分别有效、各自选出Leader的情况。举个具体示例：假设旧配置是3个节点（A、B、C），多数派需2票（旧规则）；要改成新配置5个节点（A、B、C、D、E），多数派需3票（新规则）。你可能会疑问：DE是新增节点，启动时不该直接携带新配置吗？为何会用旧配置？核心原因是「配置生效需要集群统一确认，而非单个节点独立生效」：DE启动时可能确实预置了新配置元信息，但Raft要求“配置变更必须通过集群一致性确认”，在集群还没完成统一配置更新前，DE的新配置并未真正生效（否则相当于单个节点擅自启用新规则，破坏集群一致性）。此时若直接切换，就会出现混乱：节点A、B可能提前将本地配置更新为生效的新配置（按3票多数派判定），而C仍用未失效的旧配置，新增的D、E因集群未统一确认，新配置无法生效，只能暂时沿用旧配置（或遵循未更新的集群规则），最终C、D、E都按2票多数派判定。此时集群没有统一的配置和多数派标准，若原Leader宕机，关键疑问点来了：D为何既给C（旧配置选举）投票，又给A（新配置选举）投票？核心原因是「过渡期内D的配置未明确生效，且Raft投票规则只校验“候选人日志是否更新”，不校验“配置是否统一”」——D此时还在沿用旧配置，所以会响应C的旧配置选举请求，投出一票（和C凑够旧配置多数派2票，让C当选旧配置Leader）；同时，D虽然用旧配置，但收到A的新配置选举请求时，会先校验A的日志是否更新（只要A日志达标，就会投出一票），而A按新配置规则统计票数，拿到A、B、D三票（满足新配置多数派3票），从而当选新配置Leader。这里的关键是：直接切换时没有统一的配置约束，节点会响应所有符合日志校验的选举请求，导致同一任期内出现两个Leader，各自按自己的配置同步日志、处理请求，最终导致数据混乱。

**解决办法（两阶段，无downtime）**

核心是先切到“过渡配置”，再切到新配置，全程保证不会出现双Leader，且集群能正常响应请求。

1. **第一阶段（共同一致 => 过渡配置）**

过渡配置 = 旧配置 + 新配置（两者结合），但重点是：它是集群统一确认生效的单一配置，而非直接切换时“新旧配置各自独立、混乱并存”的状态——这也是它和前面出问题场景的核心区别！直接切换时，新旧配置是“各玩各的”（A、B按新配置，C、D、E按旧配置，无统一约束）；而过渡配置是集群通过一致性算法确认的“统一规则”，所有节点都按这套合并规则工作，具体要求如下：
- 日志复制：必须同步给旧配置和新配置里的所有节点（比如旧3节点+新2节点，共5个节点都要同步）；

- 选举/日志提交：必须同时获得旧配置多数派（比如旧3节点需2票）和新配置多数派（比如新5节点需3票）的同意，缺一不可；

- 权限约束：只有同时在新旧配置里，或在其中一套配置里且符合规则的节点，才能参与决策（而非各自按规则独立决策）。

简单说，过渡配置是“有统一约束的新旧合并规则”，而直接切换是“无统一约束的新旧混乱并存”——前者能避免双Leader，后者必然出问题。等过渡配置通过“双多数派”确认提交后，Leader再同步“新配置”日志给所有节点，节点收到后立即按新配置工作（旧配置失效）。

新配置提交后，旧配置里的节点可以被关闭。

2. **第二阶段（切换新配置）**

核心是在过渡配置安全落地后，完成最终的配置切换，流程如下：1.  过渡配置提交后，Leader创建“新配置”专属日志条目，同步给集群所有节点（此时新旧配置节点都会接收，但已按过渡配置约束同步完成）；2.  每个节点收到新配置日志后，立即生效新配置（不再遵循过渡配置的双多数派规则，只按新配置的规则工作）；3.  新配置日志需按新配置的多数派标准完成提交（比如新5节点需3票确认）；4.  提交完成后，旧配置彻底失效，不在新配置中的节点（已按算法规则逐步退出）可被最终下线，配置变更完成。
新配置提交后，旧配置里的节点可以被关闭。

**三个常见问题及解决**
1. 新节点没日志：新增节点刚启动时，普遍没有集群的历史日志，所以先让新节点“旁听”（同步集群所有历史日志，但暂时没有投票权），等日志追上集群当前进度后，再让其进入过渡配置、参与正常决策；

2. Leader不在新配置里（配置变更中的常见场景）：若原Leader未被纳入新配置（即新配置移除了原Leader），则Leader在将新配置日志提交完成后，会先自动退为跟随者（避免擅自退出导致集群临时混乱）。后续之所以会停止响应、不再发送任何RPC请求，核心是新配置的硬性约束（一致性算法（Raft）本身就规定了被移除节点的退出流程）——新配置日志一旦提交，就成为集群统一遵守的规则，被移除的原Leader（已退为跟随者）会严格执行规则，主动配合退出，具体步骤如下：① 停止响应集群的RPC请求（包括日志复制的AppendEntries RPC、选举的RequestVote RPC）；② 不再发送任何RPC请求（不发起选举、不同步日志）；③ 最终可被运维关闭或自动下线，彻底脱离集群。节点之所以“听话”，是因为Raft算法规定：所有节点必须以本地日志中最新的配置作为行为准则，新配置日志提交后，节点会自动用新规则覆盖旧规则，主动执行退出相关操作，无需人工强制干预。之后集群会按新配置的规则重新选举新Leader（新Leader必是新配置内节点）；

3. 被移除的旧节点捣乱（反复发起选举）：节点收到心跳确认有Leader时，忽略旧节点的选举请求，避免当前Leader被推翻。

**一句话总结：**配置变更分两步：先“旧+新”过渡（双多数派确认），再切新配置，安全无 downtime，不会出双Leader。

---

**或者图解两阶段：**

![](/assets/05-Raft一致性算法-1769482363634.png)
![](/assets/05-Raft一致性算法-1769482350667.png)


### 日志压缩

#### 快照的基础思想

把已经提交的日志，压缩成一份 “最终状态快照”，只保留结果，丢掉过程。
![](/assets/05-Raft一致性算法-1769482677131.png)
图 12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。
![](/assets/05-Raft一致性算法-1769482993383.png)
1. 快照默认是集群中每个服务器各自独立创建的，不是 Leader 统一创建分发；
2. Leader 仅在特殊情况才会主动给跟随者发快照：当 Leader 已经删除了要发给某个跟随者的下一条日志（日志被快照覆盖后清理了），跟随者又没有这条日志，无法通过常规的日志同步追上 Leader；
- 这种特殊情况不属于集群常规操作（正常同步的跟随者都有最新日志，无需发快照）；
- 触发该情况的典型跟随者：运行极慢、长期落后的跟随者，或是新加入集群的服务器（无历史日志 / 日志缺口大）；

此时唯一能让这类跟随者快速追上集群最新状态的方式：Leader 通过网络将自己的快照发送给它们（替代日志同步，因为核心日志已经被 Leader 清理了）。

简单记：（快照用于异常/落后大的节点快速同步）快照各节点自己建，Leader 一般不用发；但如果跟随者太落后 / 是新节点，Leader 已经清掉了它需要的日志，就只能发快照让它快速同步，这是这类场景下的唯一解决方案。

---

**领导人安装快照RPC：**
![](/assets/05-Raft一致性算法-1769483367486.png)
![](/assets/05-Raft一致性算法-1769483374347.png)
1. 安装快照的基本流程
   当跟随者太落后（Leader 已删除其缺失的日志），Leader 用 InstallSnapshot RPC 发送快照。
   快照会分块传输，每块都让跟随者重置选举超时，避免因传输慢而误判 Leader 掉线。
   跟随者收到后处理本地日志：
   快照包含新信息 → 丢弃整个日志，用快照替代（包括冲突的未提交条目）。
   快照只是本地日志的前缀 → 删除快照覆盖部分，保留后面的有效日志。
2. 设计上的 “小背离”：跟随者可独立做快照
   Raft 原则是 “强 Leader”，但快照是例外：每个节点可独立创建快照。
   原因：快照只处理已提交、无冲突的数据，不需要 Leader 介入冲突解决，所以安全。
   数据流向仍由 Leader 控制（Leader 发快照给落后节点），只是跟随者可本地重组数据。
3. 为什么不采用 “Leader 统一做快照再分发”？
   带宽浪费：跟随者本地已有生成快照的全部信息，自己做比从网络接收更高效。
   实现复杂：Leader 要并行发快照和新日志，避免阻塞客户端请求，增加复杂度。
4. 快照的性能问题与解决
   （1）何时创建快照？
   策略：日志达到固定大小时创建。
   阈值设得比快照本身大很多 → 既避免频繁创建浪费资源，又防止日志无限增长。
   （2）写快照不阻塞正常操作？
   用 写时复制（Copy-on-Write）：
   状态机用函数式数据结构，或系统级 fork（如 Linux）。
   快照写入时，新更新可继续接收，互不影响。
   一句话总结
   这段讲的是：Raft 用 InstallSnapshot RPC 解决落后节点同步问题，允许节点独立做快照以提升效率，并用分块传输、写时复制、按日志大小触发等方式优化性能，同时保证一致性与安全性。

---

### 客户端交互

概念：
- 线性化语义：操作的执行顺序全局共识 + 写后立即读到最新值
官方：
![](/assets/05-Raft一致性算法-1769484929530.png)

- 客户端是如何和 Raft 进行交互的？
- 客户端如何发现领导人？
- Raft 是如何支持线性化语义的？

这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多

---

客户端找集群的领导人过程：
- 客户端启动，随机挑选集群中一个服务器发起请求；
- 若选中的是Leader：直接处理请求，流程结束；
- 若选中的是跟随者：跟随者拒绝请求，并把缓存的最新 Leader 地址返回给客户端，客户端向该地址重定向请求；
- 若选中的节点无可用 Leader 信息（如 Leader 刚崩溃，还没完成新选举）：客户端请求超时，之后重复「随机选节点」的步骤，直到找到新 Leader。

---

客户端请求的幂等性：
> 背景：领导人执行后准备响应前崩溃了，客户端响应超时，然后客户端和新的领导人再次请求，避免再次处理请求，而是直接响应结果

做法：
- 客户端对于每一条指令都赋予一个唯一的序列号。然后，（新leader的）状态机跟踪每条指令最新的
  序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重
  新执行指令

---

**问题**

Raft 中只读操作本可无日志直接处理，但无限制时会返回脏数据，因处理请求的 Leader 可能已被集群罢免却自身不知情，客户端也可能仍认为其是 Leader，最终返回的是集群已更新的旧状态，违反线性化语义。

**分析**

- Leader 虽因「Leader完全特性」持有所有已提交日志，但任期初期无法明确日志的最新提交范围，缺少集群最新的提交信息；
- 分布式网络不可靠，Leader 可能被隔离 / 心跳延迟而遭集群重新选举，却自身未感知身份失效，此时处理读请求会用旧状态返回脏数据；
- 客户端无主动探活机制，会持续向失效 Leader 发请求，加剧脏数据问题。

**解决**

为保证无日志仍实现线性化只读，Raft 增加两个核心强制措施，并提供一种可选性能优化方案：
- **任期初提交空白日志：**Leader 上任后先提交一条无实际操作的空白日志，以此确认并同步集群最新的日志提交范围，确保自身掌握的提交信息是最新的；
- **读请求前校验有效性：**Leader 处理只读请求前，先与集群多数节点交换心跳，通过多数节点的响应确认自己仍是有效 Leader，未被废黜；
- **可选优化（提升只读效率）：**基于心跳实现租约机制，租约有效期内 Leader 无需每次读请求都做心跳校验，靠有界的时间误差保证安全性，提升只读操作效率。

